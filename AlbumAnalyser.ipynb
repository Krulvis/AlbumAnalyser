{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting link <a href=\"https://yanirseroussi.com/2015/07/06/learning-about-deep-learning-through-album-cover-classification/\">Album Cover CNN </a> <a href=\"https://github.com/yanirs/bandcamp-deep-learning/\">GIT</a>\n",
    "<br />\n",
    "Caffe example: <a href=\"https://github.com/yanirs/bandcamp-deep-learning/blob/master/notebooks/02-caffe-games.ipynb\">Yanirs caffe2 pretrained model</a>\n",
    "<br />\n",
    "<a href=\"https://github.com/caffe2/models\">Caffe2 models</a>\n",
    "<br />\n",
    "<a href=\"https://github.com/caffe2/tutorials/blob/master/Loading_Pretrained_Models.ipynb\">Loading pre-trained models in Caffe2</a>\n",
    "<br/>\n",
    "<a href=\"https://caffe2.ai/docs/tutorial-MNIST.html\">CNN Caffe2 Tutorial</a>\n",
    "<br />\n",
    "<a href=\"https://towardsdatascience.com/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932\">How to use softmax output layer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import skimage.io as io\n",
    "import skimage.transform\n",
    "import operator\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "from caffe2.python import core, model_helper, net_drawer, workspace, visualize, brew\n",
    "\n",
    "core.GlobalInit(['caffe2', '--caffe2_log_level=0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still needs to download 10000 covers\n",
      "Successfully downloaded 9753/10000 images\n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "import csv\n",
    "from multiprocessing.pool import ThreadPool\n",
    "\n",
    "\n",
    "def _download_image(args):\n",
    "    session, url, local_path = args[0], args[1], args[2]\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "    if response.status_code != requests.codes.ok:\n",
    "        return False\n",
    "\n",
    "    with open(local_path, 'wb') as out:\n",
    "        out.write(response.content)\n",
    "    return True\n",
    "\n",
    "def download_dataset_images(out_dir, dataset_links_tsv='dataset-links.tsv', num_threads=10):\n",
    "    \"\"\"Download the dataset images to out_dir, based on the links in dataset_links_tsv.\n",
    "\n",
    "    dataset_links_tsv is expected to be a tab-separated file with two columns: image url, and local path with format\n",
    "    <genre>/<album_id>.jpg\n",
    "    \"\"\"\n",
    "#     if os.path.exists(out_dir):\n",
    "#         raise ValueError('%s already exists' % out_dir)\n",
    "\n",
    "    session = requests.Session()\n",
    "    with open(dataset_links_tsv) as fh:\n",
    "        jobs = []\n",
    "        genres = set()\n",
    "        for url, rel_path in csv.reader(fh, delimiter='\\t'):\n",
    "            genres.add(os.path.dirname(rel_path))\n",
    "            path = os.path.join(out_dir, rel_path)\n",
    "            if not os.path.exists(path):\n",
    "                jobs.append((session, url, os.path.join(out_dir, rel_path)))\n",
    "\n",
    "    for genre in genres:\n",
    "        genre_path = os.path.join(out_dir, genre)\n",
    "        if not os.path.exists(genre_path):\n",
    "            os.makedirs(genre_path)\n",
    "            \n",
    "    print('Still needs to download {} covers'.format(len(jobs)))\n",
    "    pool = ThreadPool(num_threads)\n",
    "    num_successes = sum(pool.map(_download_image, jobs))\n",
    "    print('Successfully downloaded %s/%s images' % (num_successes, len(jobs)))\n",
    "    \n",
    "download_dataset_images('./img/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caffe2 Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mean file found!\n",
      "mean was set to:  128\n",
      "Init net file:  /usr/lib/python3.7/site-packages/caffe2/python/models/bvlc_googlenet/init_net.pb\n",
      "Predict net file:  /usr/lib/python3.7/site-packages/caffe2/python/models/bvlc_googlenet/predict_net.pb\n"
     ]
    }
   ],
   "source": [
    "from caffe2.python import models\n",
    "models = os.path.dirname(models.__file__)\n",
    "MODEL = 'bvlc_googlenet', 'init_net.pb', 'predict_net.pb', 'ilsvrc_2012_mean.npy', 227\n",
    "IMAGE_LOCATION = \"img/ambient/a4072966.jpg\"\n",
    "codes =  \"https://gist.githubusercontent.com/aaronmarkham/cd3a6b6ac071eca6f7b4a6e40e6038aa/raw/9edb4038a37da6b5a44c3b5bc52e448ff09bfe5b/alexnet_codes\"\n",
    "\n",
    "MEAN_FILE = os.path.join(models, MODEL[0], MODEL[3])\n",
    "if not os.path.exists(MEAN_FILE):\n",
    "    print(\"No mean file found!\")\n",
    "    mean = 128\n",
    "else:\n",
    "    print (\"Mean file found!\")\n",
    "    mean = np.load(MEAN_FILE).mean(1).mean(1)\n",
    "    mean = mean[:, np.newaxis, np.newaxis]\n",
    "print(\"mean was set to: \", mean)\n",
    "\n",
    "INIT_NET = os.path.join(models, MODEL[0], MODEL[1])\n",
    "PREDICT_NET = os.path.join(models, MODEL[0], MODEL[2])\n",
    "print('Init net file: ', INIT_NET)\n",
    "print('Predict net file: ', PREDICT_NET)\n",
    "\n",
    "# Read predictor required init, predict files\n",
    "with open(INIT_NET, \"rb\") as f:\n",
    "    init_net = f.read()\n",
    "with open(PREDICT_NET, \"rb\") as f:\n",
    "    predict_net = f.read()\n",
    "    \n",
    "# Initialize the workspace predictor\n",
    "p = workspace.Predictor(init_net, predict_net)\n",
    "\n",
    "# Set the labels in a class array\n",
    "with open('codes.txt', 'r') as response:\n",
    "    class_LUT = []\n",
    "    for line in response:\n",
    "        code, result = line.partition(\":\")[::2]\n",
    "        code = code.strip()\n",
    "        result = result.replace(\"'\", \"\")\n",
    "        if code.isdigit():\n",
    "            class_LUT.append(result.split(\",\")[0][1:])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img,cropx,cropy):\n",
    "    y,x,c = img.shape\n",
    "    startx = x//2-(cropx//2)\n",
    "    starty = y//2-(cropy//2)    \n",
    "    return img[starty:starty+cropy,startx:startx+cropx]\n",
    "\n",
    "def rescale(img_location, input_height, input_width):\n",
    "    img = skimage.data.load(img_location)\n",
    "    aspect = img.shape[1]/float(img.shape[0])\n",
    "\n",
    "    if aspect > 1:\n",
    "        # landscape orientation - wide image\n",
    "        res = int(aspect * input_height)\n",
    "        imgScaled = skimage.transform.resize(img, (input_width, res))\n",
    "    elif aspect < 1:\n",
    "        # portrait orientation - tall image\n",
    "        res = int(input_width/aspect)\n",
    "        imgScaled = skimage.transform.resize(img, (res, input_height))\n",
    "    elif aspect == 1:\n",
    "        imgScaled = skimage.transform.resize(img, (input_width, input_height))\n",
    "    \n",
    "    return imgScaled\n",
    "\n",
    "# Initialize batch array\n",
    "def create_img_batch(images):\n",
    "    height, width = 224, 224\n",
    "    NCHW_batch = np.zeros((len(images),3,height,width))\n",
    "    # For each of the images in the list, format it and place it in the batch\n",
    "    for i, curr_img in enumerate(images):\n",
    "        img = rescale(curr_img, height, width)\n",
    "        img = crop_center(img, height, width)\n",
    "        img = img.swapaxes(1, 2).swapaxes(0, 1)\n",
    "        img = img[(2, 1, 0), :, :]\n",
    "        img = img * 255 - mean\n",
    "        NCHW_batch[i] = img\n",
    "    print (\"Batch Shape: \",NCHW_batch.shape)\n",
    "    return NCHW_batch\n",
    "\n",
    "#Parsing the results of one image, creating topN predictions\n",
    "def parse_results(results, topN):\n",
    "    index = 0\n",
    "    highest = 0\n",
    "    arr = np.empty((0,2), dtype=object)\n",
    "    arr[:,0] = int(10)\n",
    "    arr[:,1:] = float(10)\n",
    "    for i, r in enumerate(results):\n",
    "        # imagenet index begins with 1!\n",
    "        i=i\n",
    "        arr = np.append(arr, np.array([[i,r]]), axis=0)\n",
    "        if (r > highest):\n",
    "            highest = r\n",
    "            index = i \n",
    "            \n",
    "    topN = sorted(arr, key=lambda x: x[1], reverse=True)[:topN]\n",
    "#     print(\"Raw top {} results: {}\".format(N,topN))\n",
    "    prediction = []\n",
    "    for i, p in topN:\n",
    "        pred = (class_LUT[int(i)], round(p * 100, 2))\n",
    "        prediction.append(pred)\n",
    "    return prediction\n",
    "\n",
    "def get_labels_for_images(images):\n",
    "    \"\"\"Returns list of labels that were predicted from given images\"\"\"\n",
    "    batch = create_img_batch(images)\n",
    "    results = p.run([batch.astype(np.float32)])\n",
    "\n",
    "    # Turn it into a multi-dimensional array\n",
    "    results = np.asarray(results)\n",
    "    # Squeeze out the unnecessary axis\n",
    "    preds = np.squeeze(results)\n",
    "    print(\"Squeezed Predictions Shape, with batch size {}: {}\".format(len(images),preds.shape))\n",
    "\n",
    "    labels = []\n",
    "    for i, pred in enumerate(preds):\n",
    "        predictions = parse_results(pred, 10)\n",
    "#         print('Predictions for img: {}:'.format(images[i]))\n",
    "        for pred in predictions:\n",
    "#             print(pred)\n",
    "            labels.append(pred[0])\n",
    "    return labels\n",
    "\n",
    "def add_labels_to_genre(labels, genre):\n",
    "    for word in punk:\n",
    "        genre[word] = genre.get(word, 0) + 1\n",
    "    return genre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create output dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (86, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (79, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (76, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (75, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (71, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (82, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (81, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (74, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (73, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (100, 3, 224, 224)\n",
      "Batch Shape:  (56, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "batch_size = 100\n",
    "genres = {f:{} for f in os.listdir('img')}\n",
    "for genre in genres:\n",
    "    GENRE_LOCATION = './img/' + genre + '/'\n",
    "    FOLDER = os.listdir(GENRE_LOCATION)\n",
    "    for i in range(0, 1000, batch_size):\n",
    "        images = [os.path.abspath(GENRE_LOCATION + f) for f in FOLDER[i:i+batch_size]]\n",
    "        batch = create_img_batch(images)\n",
    "        results = p.run([batch.astype(np.float32)])\n",
    "        # Turn it into a multi-dimensional array\n",
    "        results = np.asarray(results)\n",
    "        # Squeeze out the unnecessary axis\n",
    "        preds = pd.DataFrame(np.squeeze(results))\n",
    "        preds['genre'] = genre\n",
    "        df = pd.concat([df, preds])\n",
    "# # Describe the results\n",
    "# for i, pred in enumerate(preds):\n",
    "#     print(\"Results for: '{}' :{}\".format(images[i], pred))\n",
    "#     # Get the prediction and the confidence by finding the maximum value \n",
    "#     #   and index of maximum value in preds array\n",
    "#     curr_pred, curr_conf = max(enumerate(pred), key=operator.itemgetter(1))\n",
    "#     print(\"\\tPrediction: \", curr_pred)\n",
    "#     print(\"\\tClass Name: \", class_LUT[int(curr_pred)])\n",
    "#     print(\"\\tConfidence: \", curr_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['punk', 'hiphop_rap', 'pop', 'dubstep', 'rock', 'soul', 'jazz',\n",
       "       'metal', 'folk', 'ambient'], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genre.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = df.genre\n",
    "X = df.iloc[:, 0:1000]\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "encoded_Y = keras.utils.to_categorical(encoded_Y)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, encoded_Y, test_size=0.33)\n",
    "# list(encoder.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9753, 10)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_examples_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "# Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# Model has an input_dimension of 1000 (all the words)\n",
    "model.add(Dense(64, activation='relu', input_dim=1000))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          epochs=20,\n",
    "          batch_size=128)\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test image conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_resized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-1101d55bc314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgBGR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_resized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_IMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_resized' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "img, imgBGR = get_resized(TEST_IMG, 224, 224)\n",
    "pyplot.figure()\n",
    "pyplot.subplot(1,2,1)\n",
    "pyplot.imshow(img)\n",
    "pyplot.axis('on')\n",
    "pyplot.title('Original image = RGB')\n",
    "\n",
    "pyplot.figure()\n",
    "for i in range(3):\n",
    "    # For some reason, pyplot subplot follows Matlab's indexing\n",
    "    # convention (starting with 1). Well, we'll just follow it...\n",
    "    pyplot.subplot(1, 3, i+1)\n",
    "    pyplot.imshow(imgBGR[i])\n",
    "    pyplot.axis('off')\n",
    "    pyplot.title('BGR channel %d' % (i+1))\n",
    "    \n",
    "imgCropped = imgBGR[np.newaxis, :, :, :].astype(np.float32)\n",
    "print('Final input shape is:', imgCropped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look through the downloaded Albums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4064 files were found under current folder. \n",
      "FUCK:  TM404-Dub Techno-Kontra-Musik-2013.jpg\n",
      "FUCK:  Knee-Deep In The North Sea-Jazz-Real World-2007.jpg\n",
      "FUCK:  ZGMF-X19A-UK Bass-Infinite Machine-2015.jpg\n",
      "FUCK:  Singles 93-03-Big Beat-Virgin-2003.jpg\n",
      "FUCK:  Everywhere at the end of time - Stage 4-Dark Ambient-History Always Favours The Winners-2018.jpg\n",
      "FUCK:  Acquiescence-Downtempo-Non-Projects-2012.jpg\n",
      "FUCK:  32 Zel  Planet Shrooms-Hip-Hop--2015.jpg\n",
      "FUCK:  Jenny Death-Hip-Hop-Harvest Records-2015.jpg\n",
      "FUCK:  Re-Animations EP-Footwork-Astrophonica-2013.jpg\n",
      "FUCK:  Czarface meets Metal Face-Hip-Hop-Silver Age-2018.jpg\n",
      "FUCK:  And. IV [Inertia]-Glitch-Raster-Noton-2012.jpg\n",
      "FUCK:  EP1 Drum Talking-Dub-One-Handed Music-2011.jpg\n",
      "FUCK:  Alas Rattoisaa Virtaa-Neo-Psychedelia-Dekorder-2014.jpg\n",
      "FUCK:  Old-Hip-Hop-Fool's Gold Records-2013.jpg\n",
      "FUCK:  Inner Treaty-Neo-Psychedelia-Sun Ark Records-2012.jpg\n",
      "FUCK:  volume 1 flick your tongue against your teeth and describe the present.-Hip-Hop--2017.png\n",
      "FUCK:  Plug 1 - Visible Crater Funk EP-Drum & Bass-Personal (Stereo)-1995.jpg\n",
      "FUCK:  Revolução 2005-2008-Latin Club-NOS Discos-2015.jpg\n",
      "FUCK:  Adapt-Subtechno-Tessier-Ashpool Recordings-2015.jpg\n",
      "FUCK:  Coalmine Records Presents Unearthed (Deluxe Edition)-Hip-Hop-Coalmine Records-2014.jpg\n",
      "FUCK:  Mouthed-Post-Industrial-Halcyon Veil-2016.jpg\n",
      "FUCK:  Spielraum-Glitch-Raster-Noton-2016.jpg\n",
      "FUCK:  L-Event-Glitch-Warp Records-2013.jpg\n",
      "FUCK:  Kairos Chronos-Neo-Psychedelia-Not Not Fun-2013.jpg\n",
      "FUCK:  Platinum Flame-Hip-Hop-Basedworld Records-2018.jpg\n",
      "FUCK:  The OD EP-Hip-Hop-Fool's Gold Records-2012.jpg\n",
      "FUCK:  Svans EP-Dub Techno-Kontra-Musik-2013.png\n",
      "FUCK:  Oil of Every Pearl's Un-Insides-Bubblegum Bass-Future Classic-2018.jpg\n",
      "FUCK:  Sinn + Form-Glitch-Raster-Noton-2015.jpg\n",
      "FUCK:  MISTA THUG ISOLATION-Hip-Hop-Hundebiss-2012.jpg\n",
      "FUCK:  Parallel Series 1-Techno-Mote-Evolver-2011.jpg\n",
      "FUCK:  Babilonia-UK Bass-Tessier-Ashpool Recordings-2014.jpg\n",
      "FUCK:  Rinse Presents Royal-T-Bassline House-Rinse-2012.jpg\n",
      "FUCK:  Fermi-UK Bass-Tessier-Ashpool Recordings-2015.jpg\n",
      "FUCK:  Raw As F--k-Big Beat-Against The Grain-2004.jpg\n",
      "FUCK:  The Weeping Worm-Hip-Hop--2014.jpg\n",
      "FUCK:  Shabazz Palaces-Hip-Hop-Templar Label Group-2009.jpg\n",
      "FUCK:  Fucked Up Friends-Neo-Psychedelia-Anticon-2008.jpg\n",
      "FUCK:  Hora de Balako-Disco-Greco-Roman-2017.jpg\n",
      "FUCK:  A Sufi And A Killer-Neo-Psychedelia-Warp Records-2010.jpg\n",
      "FUCK:  Contra-Hip-Hop-Sony Music-2017.jpg\n",
      "FUCK:  OD-Axis-Berlin School-Digitalis Recordings-2011.jpg\n",
      "FUCK:  midcity-Hip-Hop--2013.jpg\n",
      "FUCK:  Music To Moog By-Space Age Pop-Wah-Wah Records-1969.jpg\n",
      "FUCK:  If Your Girl Only Knew-Outsider House-E-Beamz-2017.jpg\n",
      "FUCK:  Doris-Hip-Hop-Odd Future-2013.jpg\n",
      "FUCK:  Cliff Of Death II-Hip-Hop--2015.jpg\n",
      "FUCK:  Sleepstep-Ambient-Raster-Noton-2015.jpg\n",
      "FUCK:  Elävänä planeetalla-Neo-Psychedelia-New Images-2011.jpg\n",
      "FUCK:  Bottomless Pit-Hip-Hop-Third Worlds-2016.jpg\n",
      "FUCK:  Fusion in Park-Acid-Kontra-Musik-2014.jpg\n",
      "FUCK:  Pastel Love - Part One-Balearic-Back To The Balearics-2013.jpg\n",
      "FUCK:  Every Hero Needs A Villain-Hip-Hop-Brick Records-2015.jpg\n",
      "FUCK:  Still Brazy-Hip-Hop-Def Jam Recordings-2016.jpg\n",
      "FUCK:  Sunsets and Clocks EP-Downtempo-Non-Projects-2013.jpg\n",
      "FUCK:  History Will Absolve Me-Hip-Hop-Backwoodz Studioz-2012.jpg\n",
      "FUCK:  Dot-Bubbles-Chiptune-Lo-Bit Playground Recordings-2006.jpg\n",
      "FUCK:  Graviton - Cx (Rigid Transmission)-Glitch-Semantica Records-2011.jpg\n",
      "FUCK:  IRISIRI-Post-Industrial-PAN-2018.jpg\n",
      "FUCK:  Reflections - Mojave Desert-Psychedelic Rock-Luaka Bop-2017.jpg\n",
      "FUCK:  Electronic Minimal Music, 1979 - 1983-Berlin School-Black Sweat Records-2015.jpg\n",
      "FUCK:  SQ777-7 Sacred Code (Encrypted)-Industrial-Asura Revolver-2017.jpg\n",
      "FUCK:  Mo7it Al-Mo7it-World Music-Constellation Records-2013.jpg\n",
      "FUCK:  La Demeure; Il Y A Péril En La Demeure-Glitch-Raster-Noton-2015.jpg\n",
      "FUCK:  good kid, m.A.A.d city-Hip-Hop-Aftermath-2012.jpg\n",
      "FUCK:  Indigoism-Hip-Hop-Brainfeeder-2013.jpg\n",
      "FUCK:  Atrocity Exhibition-Hip-Hop-Warp Records-2016.jpg\n",
      "FUCK:  Run the Jewels-Hip-Hop-Fool's Gold Records-2013.jpg\n",
      "FUCK:  Imperial-Hip-Hop--2016.jpg\n",
      "FUCK:  Sweatbox Dynasty-Neo-Psychedelia-Rad Cult-2016.jpg\n",
      "FUCK:  Asura-Downtempo-Non-Projects-2010.jpg\n",
      "FUCK:  BUST A SAG DA SINGLE-Hip-Hop--2011.jpg\n",
      "FUCK:  200 Press-Future Garage-1-800-Dinosaur-2014.jpg\n",
      "FUCK:  Mobis-Hip-Hop--2016.png\n",
      "FUCK:  So It Goes-Hip-Hop-Hot Charity-2014.jpg\n",
      "FUCK:  36-26-36-Minimal-Convex Industries-2013.jpg\n",
      "FUCK:  Acidub-Dub Techno-Kontra-Musik-2016.jpg\n",
      "FUCK:  Believe EP-UK Bass-1-800-Dinosaur-2014.jpg\n",
      "FUCK:  Sunburn EP-House-Greco-Roman-2012.png\n",
      "FUCK:  Belomancie-Neo-Psychedelia-Drag City-2014.jpg\n",
      "FUCK:  SQ777-5 Modern Esoteric-Industrial-House of the Leg-2017.jpg\n",
      "FUCK:  niggas on the moon-Hip-Hop-Harvest Records-2014.jpg\n",
      "FUCK:  -Hip-Hop-Bad Vibes Forever-2018.jpg\n",
      "FUCK:  Year Of The Snitch-Hip-Hop-Third Worlds-2018.jpg\n",
      "FUCK:  SQ777-2 Heel Aesthetic-Industrial-H.V.R.F. Central Command-2017.jpg\n",
      "FUCK:  Background Noise-Tribal-Kontra-Musik-2018.jpg\n",
      "FUCK:  NO LOVE DEEP WEB-Hip-Hop-Harvest Records-2012.jpg\n",
      "FUCK:  RN Rhythm-Variations -Glitch-Raster-Noton-2009.jpg\n",
      "FUCK:  Know Thyself-Neo-Psychedelia-Sun Ark Records-2012.jpg\n",
      "FUCK:  Shimmer-Tech House-Tessier-Ashpool Recordings-2014.jpg\n",
      "FUCK:  13-Hip-Hop-PH Recordings-2017.jpg\n",
      "FUCK:  Ascension-Neo-Psychedelia-Not Not Fun-2010.jpg\n",
      "FUCK:  Haze EP-Outsider House-E-Beamz-2016.jpg\n",
      "FUCK:  Playground-Tech House-Greco-Roman-2014.jpg\n",
      "FUCK:  SQ777-1 No Man Is God-Industrial-BLCR Laboratories-2017.jpg\n",
      "FUCK:  Black Up-Hip-Hop-Sub Pop-2011.jpg\n",
      "FUCK:  CLPPNG-Hip-Hop-Sub Pop-2014.jpg\n",
      "FUCK:  Exmilitary-Hip-Hop-Grindcore Karaoke-2011(1).jpg\n",
      "FUCK:  Urban Deity-UK Bass-Tessier-Ashpool Recordings-2014.jpg\n",
      "FUCK:  NoYork!-Hip-Hop-Nature Sounds-2011.jpg\n",
      "FUCK:  Jonwayne is Retired-Hip-Hop-Authors-2015.jpg\n",
      "FUCK:  Czarface-Hip-Hop-Brick Records-2013.jpg\n",
      "FUCK:  Mystic Thickness-Neo-Psychedelia-Anticon-2010.jpg\n",
      "FUCK:  Maniac Meat-Neo-Psychedelia-Anticon-2010.JPG\n",
      "FUCK:  Drum Vehicle-UK Bass-Tessier-Ashpool Recordings-2014.jpg\n",
      "FUCK:  The Ziggurat-Hip-Hop-African Dope-2003.jpg\n",
      "FUCK:  Trarat-Neo-Psychedelia-Leaving Records-2016.jpg\n",
      "FUCK:  Plug 2 - Rebuilt Kev EP-Drum & Bass-Personal (Stereo)-1995.jpg\n",
      "FUCK:  Quanta-Tech House-Tessier-Ashpool Recordings-2015.jpg\n",
      "FUCK:  GalaXXXian-Hip-Hop--2014.png\n",
      "FUCK:  Sexualizer - EP-Synthwave-Aphasia Records-2013.jpg\n",
      "FUCK:  Bromance #12 Baby F-16-Tech House-Bromance Records-2013.jpg\n",
      "FUCK:  SQ777-6 16-Cell-Industrial-SHUT_INS-2017.jpg\n",
      "FUCK:  Trans-Neptunian Objects-Drone-Auxiliary-2017.jpg\n",
      "FUCK:  The Money Store-Hip-Hop-Epic Records-2012.jpg\n",
      "FUCK:  Zwart-Hip-Hop-[PIAS] Recordings-2018.jpg\n",
      "FUCK:  Confusion  Re-Mover-Acid--1995.jpg\n",
      "FUCK:  Rejovich-Hip-Hop-Kaya Kaya Records-2013.jpg\n",
      "FUCK:  Nostalgic 64-Hip-Hop--2013.jpg\n",
      "FUCK:  6 Donnies-Hip-Hop--2015.jpg\n",
      "FUCK:  Welcome to Paradise (Italian Dream House 89-93)-House-Safe Trip-2017.jpg\n",
      "FUCK:  Unieqav-Glitch-Raster-Noton-2018.jpg\n",
      "FUCK:  14 Tracks Re-Wiring UK Garage-Garage-Boomkat Editions-2009.jpg\n",
      "FUCK:  Kids See Ghosts-Hip-Hop-Def Jam Recordings-2018.jpg\n",
      "FUCK:  Government Plates-Hip-Hop--2013.jpg\n",
      "FUCK:  TMA-1 EP-IDM-Space Cadets-2012.jpg\n",
      "FUCK:  Everywhere at the end of time - Stage 5-Dark Ambient-History Always Favours The Winners-2018.jpg\n",
      "FUCK:  A Piece of the Geto-Hip-Hop-Ghostly International-2017.jpg\n",
      "FUCK:  On Patrol-Neo-Psychedelia-Not Not Fun-2010.jpg\n",
      "FUCK:  Ultima II Massage-Neo-Psychedelia-Ghostly International-2014.jpg\n",
      "FUCK:  Insen-Ambient-Raster-Noton-2005.jpg\n",
      "FUCK:  Perfection-UK Bass-Tessier-Ashpool Recordings-2015.jpg\n",
      "FUCK:  New Bermuda-Metal-ANTI--2015.jpg\n",
      "FUCK:  Everywhere at the end of time - Stage 3-Ambient-History Always Favours The Winners-2017.jpg\n",
      "FUCK:  Rendez-Vous-Berlin School-Polydor-1986.jpg\n",
      "FUCK:  XXX-Hip-Hop-Fool's Gold Records-2011.jpg\n",
      "FUCK:  Mat Oput - 12-Industrial-Downwards-2009.jpg\n",
      "FUCK:  The Black Brad Pitt-Hip-Hop--2012.jpg\n",
      "FUCK:  Wit-Hip-Hop--2017.jpg\n",
      "FUCK:  Run The Jewels 2-Hip-Hop-Mass Appeal-2014.jpg\n",
      "FUCK:  Trans-80-Balearic-LN-CC Recordings-2013.jpg\n",
      "FUCK:  Exmilitary-Hip-Hop-Grindcore Karaoke-2011.jpg\n",
      "FUCK:  SECRET RAVE 01-Big Beat-art-aud-2015.jpg\n",
      "FUCK:  Capsize Recovery-Industrial-Raster-Noton-2013.jpg\n",
      "FUCK:  Fallout EP-Deconstructed Club-Sci-Fi & Fantasy-2013.jpg\n",
      "FUCK:  The Roots Of El-B-Garage-Tempa-2009.jpg\n",
      "FUCK:  BetterOffDEAD-Hip-Hop-Electric KoolAde-2013.png\n",
      "FUCK:  The Money Store-Hip-Hop-Epic Records-2012(1).jpg\n",
      "FUCK:  If I Am This Forest-Chillwave-Non-Projects-2011.JPG\n",
      "FUCK:  Raw Money Raps-Hip-Hop-Brainfeeder-2012.jpg\n",
      "FUCK:  VooDoo-U Stript-Big Beat-American Recordings-1994.png\n",
      "FUCK:  Panic Blooms-Neo-Psychedelia-Rad Cult-2018.jpg\n",
      "FUCK:  SATURATION-Hip-Hop--2017.jpg\n",
      "FUCK:  All in the Circle-Hip-Hop-World Of Wils-2015.jpg\n",
      "FUCK:  Silver Trees  Damiel-Downtempo-Non-Projects-2010.jpg\n",
      "FUCK:  Contrapposto-UK Bass-Tessier-Ashpool Recordings-2015.jpg\n",
      "FUCK:  Dandelion Gum-Neo-Psychedelia--2007.jpg\n",
      "FUCK:  Of Light-Hip-Hop-Templar Label Group-2009.jpg\n",
      "FUCK:  HD-Glitch-Raster-Noton-2013.jpg\n",
      "FUCK:  The Paris-Lagos Soundscapes-Deep House-Phonogramme-2017.jpg\n",
      "FUCK:  Summvs-Ambient-Raster-Noton-2011.jpg\n",
      "FUCK:  The Blue Quicksand Is Going Now-Post-Industrial-Loopy-2015.jpg\n",
      "FUCK:  Night Voyage Tool Kit - EP-Bassline House-Sound Pellegrino-2011.jpg\n",
      "FUCK:  Purple City  Re-Up-Dubstep-Kapsize-2009.jpg\n",
      "FUCK:  Univrs-Glitch-Raster-Noton-2011.png\n",
      "FUCK:  Big Fish Theory-Hip-Hop-Def Jam Recordings-2017.jpg\n",
      "FUCK:  Exorcise Tape-Neo-Psychedelia-Rad Cult-2013.jpg\n",
      "FUCK:  Zuology-Hip-Hop--2012.jpg\n",
      "FUCK:  From Patterns to Details-Post-Industrial-Subtext-2016.jpg\n",
      "FUCK:  SATURATION 3-Hip-Hop-Empire-2017.jpg\n",
      "FUCK:  Duality -Hip-Hop-Brainfeeder-2012.jpg\n",
      "FUCK:  Over-Under EP-Deep House-Other People-2014.jpg\n",
      "FUCK:  SQ777-4 High Kinetic Energy-Industrial-Antifur-2017.jpg\n",
      "FUCK:  Dread Risk  U-Eff-O-Drum & Bass-Diagonal Records-2017.jpg\n",
      "FUCK:  Dang-Neo-Psychedelia-Leaving Records-2016.jpg\n",
      "FUCK:  EARL-Hip-Hop-Odd Future-2010.jpg\n",
      "FUCK:  Punish, Honey-Post-Industrial-Tri Angle-2014.jpg\n",
      "FUCK:  Voodoo Of The Godsent-Dub-On-U Sound-2011.jpg\n",
      "FUCK:  Everywhere at the end of time - Stage 1-Ambient-History Always Favours The Winners-2016.jpg\n",
      "FUCK:  Growing Wild-Neo-Psychedelia-Leaving Records-2015.jpg\n",
      "FUCK:  SATURATION 2-Hip-Hop--2017.jpg\n",
      "FUCK:  Les Fleurs Du Mal-Post-Industrial-Halcyon Veil-2017.jpg\n",
      "FUCK:  Lunatics Revenge-Psychedelic Rock-Sulatron-records-2016.jpg\n",
      "FUCK:  Unitxt-Glitch-Raster-Noton-2008.jpg\n",
      "FUCK:  De Party Van Viezigheid Mixtape-Hip-Hop-Noah's Ark-2017.jpg\n",
      "FUCK:  Scarlet Jungle EP-Hip-Hop-Eglo Records-2013.jpg\n",
      "FUCK:  Ballard-Industrial-Mote-Evolver-2015.jpg\n",
      "FUCK:  Emphasis-Drum & Bass-Tessier-Ashpool Recordings-2015.jpg\n",
      "FUCK:  Plug 3 - Versatile Crib Funk EP-Drum & Bass-Personal (Stereo)-1995.jpg\n",
      "FUCK:  Sonderr - Goin Home EP-Outsider House-Lost Palms-2017.jpg\n",
      "FUCK:  Small VVorld-Hip-Hop--2014.jpg\n",
      "FUCK:  ~Treibgut-Glitch-Raster-Noton-2009.jpg\n",
      "FUCK:  Binnenstad-Hip-Hop-Yellinek-2015.jpg\n",
      "FUCK:  Infra-Dusk  Infra-Dawn-Deconstructed Club-Black Ocean-2015.jpg\n",
      "FUCK:  2012 - 2017-Deep House-Other People-2018.jpg\n",
      "FUCK:  The Face EP-Future Garage-Greco-Roman-2012.jpg\n",
      "FUCK:  SQ777-3 Emotionless Passion-Industrial-KUDATAH-2017.jpg\n",
      "FUCK:  Everywhere at the end of time - Stage 2-Ambient-History Always Favours The Winners-2017.jpg\n",
      "FUCK:  Death Grips-Hip-Hop--2011.jpg\n",
      "FUCK:  Xerrox Vol.3-Drone-Raster-Noton-2015.jpg\n",
      "FUCK:  I Don't Like Shit, I Don't Go Outside An Album by Earl Sweatshirt-Hip-Hop-Tan Cressida-2015.jpg\n",
      "FUCK:  RPG  Man Like Me-Grime-1-800-Dinosaur-2016.jpg\n",
      "We start with 77 genres which are definitely too many labels for a CNN at this point.\n"
     ]
    }
   ],
   "source": [
    "class Album:\n",
    "    \n",
    "    def __init__(self, path, infos):\n",
    "        self.path = path\n",
    "        self.name = infos[0]\n",
    "        self.genre = infos[1]\n",
    "        self.label = infos[2]\n",
    "        self.year = infos[3]\n",
    "\n",
    "def read_files(file_list):\n",
    "    albums = []\n",
    "    fucked = []\n",
    "    for i in range(len(file_list)):\n",
    "        file_name = file_list[i]\n",
    "        split = file_name.split('-')\n",
    "        if file_name.count('-') >= 4:\n",
    "            print('FUCK: ', file_name)\n",
    "            fucked.append(file_name)\n",
    "        else:\n",
    "            albums.append(Album(os.path.abspath(file_name), split))\n",
    "    return albums, fucked\n",
    "    \n",
    "\n",
    "def explore_files():\n",
    "    current_folder_filename_list = os.listdir(\"./data\") # Change this PATH to traverse other directories if you want.\n",
    "    albums, fucked = [], []\n",
    "    if current_folder_filename_list != None:\n",
    "        print(\"%s files were found under current folder. \" % len(current_folder_filename_list))\n",
    "        albums, fucked = read_files(current_folder_filename_list)\n",
    "    return albums, fucked\n",
    "\n",
    "albums, fucked = explore_files()\n",
    "genres = [a.genre for a in albums]\n",
    "\n",
    "unique_genres = sorted(set(genres))\n",
    "print(\"We start with {} genres which are definitely too many labels for a CNN at this point.\".format(len(unique_genres)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clarifai Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clarifai.rest import ClarifaiApp\n",
    "app = ClarifaiApp()\n",
    "\n",
    "m = app.models.get('aaa03c23b3724a16a56b629203edc62c')\n",
    "\n",
    "# app.tag_images('http://assets.rollingstone.com/assets/images/list/0e099b2214b1673fc76c6c60257b88aefe571def.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiphop_rap\n",
      "dubstep\n",
      "rock\n",
      "ambient\n",
      "pop\n",
      "punk\n",
      "jazz\n",
      "folk\n",
      "soul\n",
      "metal\n"
     ]
    }
   ],
   "source": [
    "def get_albums_img(main_folder):\n",
    "    img_folder_list = os.listdir(main_folder) # Change this PATH to traverse other directories if you want.\n",
    "    albums = {}\n",
    "    for folder in img_folder_list:\n",
    "        print(folder)\n",
    "        if folder not in albums.keys():\n",
    "            albums[folder] = []\n",
    "        # Retrieve images for this album\n",
    "        images = os.listdir(main_folder + '/' +folder)\n",
    "        for image in images:\n",
    "            if image not in albums[folder]:\n",
    "                albums[folder].append(image)\n",
    "                \n",
    "    return albums\n",
    "\n",
    "albums = get_albums_img(\"./img\")\n",
    "\n",
    "def get_absolute_image_path(genre, image):\n",
    "    return os.path.abspath(\"./img/{}/{}\".format(genre, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/krulvis/PycharmProjects/AlbumAnalyser/img/ambient/a1604573766.jpg\n"
     ]
    }
   ],
   "source": [
    "tryout = albums['ambient'][0:1]\n",
    "for img in tryout:\n",
    "    path = get_absolute_image_path('ambient', img)\n",
    "    tags = m.predict('http://assets.rollingstone.com/assets/images/list/0e099b2214b1673fc76c6c60257b88aefe571def.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
